{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation Notebook for Fine-tuning Llama 3.1-8B-Instruct\n"
      ],
      "metadata": {
        "id": "SNF0FDjfW7Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/zackproser/portfolio"
      ],
      "metadata": {
        "id": "22HWz1b1WxSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from markdown import markdown\n",
        "from bs4 import BeautifulSoup\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "tE1NTKDQXGN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Define helper functions (from clean-markdown-corpus.py)\n",
        "def markdown_to_text(markdown_string):\n",
        "    \"\"\"Converts a markdown string to plain text.\"\"\"\n",
        "    html = markdown(markdown_string)\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text by removing extra whitespace and normalizing.\"\"\"\n",
        "    # Remove empty lines and excess whitespace\n",
        "    cleaned_lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "\n",
        "    # Break down long lines into sentences\n",
        "    cleaned_text = []\n",
        "    for line in cleaned_lines:\n",
        "        if len(line) > 200:  # Example threshold for long lines\n",
        "            sentences = re.split(r'(?<=[.!?]) +', line)\n",
        "            cleaned_text.extend(sentences)\n",
        "        else:\n",
        "            cleaned_text.append(line)\n",
        "\n",
        "    return \"\\n\".join(cleaned_text)\n",
        "\n",
        "# Cell 3: Process MDX files\n",
        "def process_mdx_files(directory):\n",
        "    \"\"\"Process all MDX files in the given directory and its subdirectories.\"\"\"\n",
        "    processed_content = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.mdx'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    raw_text = f.read()\n",
        "                plain_text = markdown_to_text(raw_text)\n",
        "                cleaned_text = clean_text(plain_text)\n",
        "                processed_content.append(cleaned_text)\n",
        "    return processed_content\n",
        "\n",
        "def extract_metadata(content):\n",
        "    \"\"\"Extract metadata from the content.\"\"\"\n",
        "    metadata = {}\n",
        "    metadata_match = re.search(r'export const metadata = createMetadata\\((.*?)\\)', content, re.DOTALL)\n",
        "    if metadata_match:\n",
        "        metadata_str = metadata_match.group(1)\n",
        "        # Extract key-value pairs\n",
        "        for match in re.finditer(r'(\\w+):\\s*[\"\\']?(.*?)[\"\\']?,?\\s*(?=\\w+:|$)', metadata_str):\n",
        "            key, value = match.groups()\n",
        "            metadata[key.strip()] = value.strip().strip('\"').strip(\"'\")\n",
        "    return metadata\n",
        "\n",
        "def clean_content(content):\n",
        "    \"\"\"Clean the content by removing imports, exports, metadata, and special characters.\"\"\"\n",
        "    # Remove import statements\n",
        "    content = re.sub(r'import.*\\n', '', content)\n",
        "    # Remove export statements and metadata\n",
        "    content = re.sub(r'export const metadata.*?}\\)', '', content, flags=re.DOTALL)\n",
        "    content = re.sub(r'export default.*\\n', '', content)\n",
        "    # Remove special characters and markdown syntax\n",
        "    content = re.sub(r'[#*`]', '', content)\n",
        "    # Remove empty lines and lines that look like object properties\n",
        "    content = '\\n'.join(line for line in content.split('\\n') if line.strip() and not re.match(r'^\\w+:\\s', line))\n",
        "    return content.strip()"
      ],
      "metadata": {
        "id": "HpDRbmErWyOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkDk_g1ZWuw8"
      },
      "outputs": [],
      "source": [
        "# Process MDX files in the portfolio directory\n",
        "mdx_content = process_mdx_files('portfolio')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_alpaca_entry(content):\n",
        "    \"\"\"Create an Alpaca format entry for the given content.\"\"\"\n",
        "    metadata = extract_metadata(content)\n",
        "    cleaned_content = clean_content(content)\n",
        "\n",
        "    article_name = metadata.get('title', 'an article').strip('\"')\n",
        "\n",
        "    return {\n",
        "        \"instruction\": f\"Write an article about \\\"{article_name}\\\"\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": cleaned_content\n",
        "    }"
      ],
      "metadata": {
        "id": "bSYWf-f2WwJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_data = [create_alpaca_entry(content) for content in mdx_content]"
      ],
      "metadata": {
        "id": "u6Sd31HpX5pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Alpaca format data to a file\n",
        "with open('training_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(alpaca_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Processed {len(alpaca_data)} entries and saved to training_data.json\")\n",
        "\n",
        "# Display sample entries (for verification)\n",
        "print(\"Sample entries:\")\n",
        "for entry in alpaca_data[:3]:  # Display first 3 entries\n",
        "    print(json.dumps(entry, indent=2))\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "id": "uQl1PJfHYCdH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}